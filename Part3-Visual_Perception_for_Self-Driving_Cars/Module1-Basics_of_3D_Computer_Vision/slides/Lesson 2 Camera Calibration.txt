So far, we have learnt which camera parameters are needed for projective
geometry to work. In this video, we
will learn how to get these camera parameters using the mathematical tools
of calibration. Let's remember the projection equations we've learnt so far. The homogeneous
coordinates of point O in 3D space can be transformed
to the camera plane, with the camera
projection matrix P, which includes both extrinsic
and extrinsic parameters. Remember, the
projected coordinates need to be converted to homogeneous form
to get the u and v pixel locations in
pixel coordinates. We do this by dividing the image coordinates by the z-component. Finally, u and v can
then be multiplied with an arbitrary scale
s. We multiply by s, as it will be useful
later on when we formulate the
calibration problem. It is important to
note that scale plays a challenging role in understanding monocular
image information, as once points are projected from the 3D world onto
the 2D image plane, scale information is lost. Points in 3D space along
a ray from the camera center, all project to the same location
on the image plane, and it is therefore not
possible to directly associate a depth to a point given
only image information. The camera calibration
problem is defined as finding these unknown intrinsic and
extrinsic camera parameters, shown here in red given n known 3D point coordinates and
their corresponding projection to the image plane. Our approach will
comprise of getting the P matrix first and
then decomposing it into the intrinsic parameters K and the extrinsic rotation parameters R and translation parameters t. For calibration, we use a scene with
known geometry to get the location of our
3D points from the 2D image, resolving the scale
issue by measuring the actual 3D distance between the points that
are observed in the image. The most commonly used example would be a 3D checkerboard, with squares of
known size providing a map of fixed point
locations to observe. We define our word
coordinate frame, in yellow and compute our 3D point coordinates and
their projections in the image. Associating 3D points to 2D projections can be
done either manually, by clicking on the purple points, for example or automatically, with checkerboard detectors. We can then set up
a system of equations to solve for the unknown
parameters of P. Now, let us form the system of linear equations that
needs to be solved. First, we expand
the projection equations to three equations through
matrix multiplication. To get zero on the right-hand
side of these equations, we move the right hand side to the left-hand side for each one. Then, we substitute
the third equation into equations one and two, and end up with
two equations per point. Therefore, if we have n points, we have 2n associated equations. Putting these equations
in matrix form gives us the shown homogeneous
linear system. Since this is a
homogeneous linear system, we can use the pseudo-inverse
or even better, the singular value decomposition to get the least
squares solution. Our simple linear
calibration approach has several advantages. It's easy to formulate, has a closed form solution, and often provides
really good initial points for non-linear
calibration approaches. Can you think of
some disadvantages of a simple linear system? One disadvantage
of solving for P, is that we do not directly get the intrinsic and extrinsic
camera parameters. Furthermore,
our linear model does not take into account
complex phenomena, such as radial and
tangential distortion. Finally, since we are solving via the linear least
squares method, we cannot impose constraints
on our solution, such as requiring the focal
length to be non-negative. The camera projection
matrix P by itself, is useful for projecting
3D points into 2D, but it has several drawbacks. It doesn't tell you
the cameras pose and it doesn't tell you about
the camera's internal geometry. Fortunately, we can
factorize P into intrinsic parameter matrix K and extrinsic rotation parameters R and translation parameters t, using a linear algebra operation known as the RQ factorization. Let us see how we perform
this factorization. First, we alter
the representation of P to be a function of
the camera center C. C is the point that projects to
zero when multiplied by P. We multiply K into the matrix
to form two sub-matrices, KR and minus KRC. We will refer to the combination of K and R as the M matrix. We can now express our projection matrix P as M and minus MC. From here, we use the fact
that any square matrix can be factored into an upper
triangular matrix R and an orthogonal basis
to decompose M into upper triangular R
and orthogonal basis Q. In linear algebra, this procedure is known as RQ factorization, which is a variant
of the more commonly referred to QR factorization. In QR factorization, we have the orthogonal Q
first and then the upper triangular
R. Note here that the R and the output
of RQ factorization, is a different variable than
our rotation matrix R. So, don't get those confused. Let's now see how we
can use the output of RQ factorization of
the matrix M to retrieve K, R, and t by aligning
these two expressions. The intrinsic calibration matrix
K is the output R of the RQ factorization of M. The rotation matrix R is
the orthogonal basis Q. Finally, we can extract the translation vector
directly from K in the last column
of the P matrix. RQ factorization is
a great tool to compute K, R, and t from
the camera P matrix. However, some mathematical
assumptions need to be performed to guarantee a unique solution
for these matrices. We will explore
these assumptions in further detail with this lesson's practice
Jupiter notebook. Monocular camera calibration is a well-established tool that has excellent implementations
in C++, Python and MATLAB. You can test out some of the most common
implementations by following the links we've included in
the supplemental materials. So, to summarize. In this lesson,
you've learned that the camera projection
matrix P can be found through a process
known as camera calibration. You've learnt that
this matrix can be factored into the
camera intrinsic matrix K and the camera's
extrinsic parameters R and t, through
RQ factorization. Next up, we'll discuss
how to get depth from two camera sensors through stereovision. We'll see you then.