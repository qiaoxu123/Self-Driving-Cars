If you've been monitoring the latest news on
self-driving cars, you would have heard the phrase convolutional
neural networks or ConvNets for
short at least a few times. In fact, we currently use ConvNets to perform
a multitude of perception tasks on our own self-driving car
the autonomoose. In this lesson, we will
take a deeper look at these fascinating
architectures to understand their importance
for visual perception. Specifically, you will learn how convolutional layers
use cross-correlation instead of general
matrix multiplication to tailor neural networks
for image input data. We'll also cover
the advantages these models incur over standard
feed-forward neural networks. Convolutional neural networks
are a specialized kind of neural network for
processing data that has a known
grid-like topology. Examples of such data can be 1D time-series data sampled
at regular intervals, 2D images or even 3D videos. ConvNets are mainly comprised
of two types of layers; convolutional layers
and pooling layers. A simple example of a convNet
architecture is VGG 16. This network takes
in the image and passes it through a set
of convolutional layers, a pooling layer, and another couple of
convolutional layers, and then more pooling layers and convolutional layers and so on. Don't worry too much
about the specifics of the VGG 16 architecture
design for now, we will discuss
this architecture in detail in a later video when we learn about
object detection. Let's see how these two types
of layers work in practice. The neural network, hidden
layers we have described so far are usually called
fully connected layers. As their name suggests, fully connected layers connect each node output to every
node input in the next layer. That means that
every element of the input contributes to every element
of the output. This is implemented in software through dense matrix
multiplication. Although counter-intuitive,
convolutional layers use cross-correlation
not convolutions for their linear operator instead of general matrix multiplication. The logic behind using cross-correlation is that if
the parameters are learned, it does not matter if we
flip the output or not. Since we are learning the weights of the convolutional layer, the flipping does not
affect our results at all. This results in what we
call sparse connectivity. Each input element to the convolutional layer only affects a few output elements, thanks to the use of a limited size kernel for
the convolutional operation. Let's begin by describing how convolutional layers
work in practice. We'll assume that
we want to apply a convolutional layer
to an input image. We will refer to this image
as our input volume, as we will see
convolutional layers taking output of other layers
as their inputs as well. The width of our input volume is its horizontal dimension, the height is
its vertical dimension, and the depth is
the number of channels. In our case, all
three characteristics have a value of three. But why didn't we consider the gray pixels in our
height or width computation? The gray pixels are added to the image through
a process called padding. The number of pixels
added on each side is called the padding
size in this case one. Padding is essential
for retaining the shape required to
perform the convolutions. We perform the convolution
operations through a set of kernels or filters. Each Filter is
comprised of a set of weights and a single bias. The number of channels
of the kernel needs to correspond to the number of
channels of the input volume. In this case, we have three weight channels per
filter corresponding to red, green, and blue channels
of the input image. Usually we have multiple filters
per convolutional layer. Let's see how to
apply our two filters to get an output volume
from our input volume. We start by taking each channel
of the filter and perform cross-correlation
between that channel and its corresponding channel
in the input volume. We then proceed to add the output of
the cross-correlation of all channels with the bias of the filter to arrive at
the final output value. Notice that we get
one output channel per filter. We will get back to
this point in a bit. Let's now see how to get
the rest of the output volume. After we're done with
the first computation, we shift the filter location by a preset number of
pixels horizontally. When we reach the end of
the input volume width, we shift the filter location by a preset number of
pixels vertically. The vertical and
horizontal shifts are usually the same value, and we refer to this value as the stride of
our convolutional layer. We arrive to a final output
volume with its own width, depth, and height values. Assuming that
the filters are M by M, and we have K filters,
a stride of S, a padding P, we can derive
expressions for the width, height, and depth of
our output volume. You would think this gets
challenging to keep track of, but when designing ConvNets, it is very important to know what size output layers
you'll end up with. As an example, you don't want to reduce the size of
your output volume too much if you are trying to
detect small traffic signs and traffic lights
on road scenes. They only occupy a small number
of pixels in the image, and their visibility
might get lost if the output volume is too compact. Let us now continue to describe the second building block of
ConvNets, pooling layers. A pooling layer uses pooling functions to replace
the output of the previous layer with a summary statistic of
the nearby outputs. Pooling helps make
the representations become invariant to
small translations of the input. If we translate the input
a small amount, the output of the pooling layer
will not change. This is important for
object recognition for example, as if we shift a car a small
amount in an image space, it should still be
recognizable as a car. Let us take an example of the most commonly used
pooling layer, Max pooling. Max pooling summarizes
output volume patches with the max function. Given the input volume in gray, Max Pooling applies the max
function to an M by M region, then shifts this region in strides similar to the
convolutional layer. Once again we can derive expressions for the output width, height, and depth of the pooling layers according
to the following equations. In our previous example, the filter size M is two, and we get a stride of two, so we end up with
a two-by-two output. Let's see how this
pooling layer can help us with translation invariance. As an example, let's shift the previous input volume
by one pixel. The added pixels due to
the shift are shown in blue, whereas the removed pixels
are shown in red. We can go ahead and apply Max Pooling to this input volume, as we did in the previous slide. When comparing our new output to the original volume output, we find that only one
element has changed. So far, we've discussed how ConvNets operate
but still did not provide a reason for their usefulness in the context
of self-driving cars. There are really
two important reasons for the effectiveness
of ConvNets. First, they usually have
far fewer parameters in their convolutional layers than a similar network with
fully connected layers. This reduces the chance of over-fitting through
parameters sharing, and allows ConvNets to
operate on larger images. Perhaps more importantly,
is translation invariance. By using the same parameters to process every block of the image, ConvNets are capable of detecting an object
or classifying a pixel even if it is shifted with a translation
on the image plane. This means we can detect
cars wherever they appear. Before we end this lesson, I would like to shed light on the history of convolutional
neural networks. Convolutional neural
networks have played an important role in
the history of deep learning. As a matter of
fact, ConvNets were one of the first
neural network models to perform well at a time where other feed-forward
architectures failed, particularly on
image classification tasks related to the ImageNet dataset. In many ways, ConvNets carry the torch for the rest
of deep learning, and pave the way to the relatively new acceptance of neural networks in general. Finally, convolutional
networks were some of the first neural networks to solve important
commercial applications, the most famous being Yann LeCun's handwritten
digit recognizer in the early nineties, and remain at the forefront of commercial applications
of deep learning today. In fact, in the next week
of this course, we will show you how
to use ConvNets to detect a range of different
objects in roads scenes, 2D object detection.
We'll see you then.