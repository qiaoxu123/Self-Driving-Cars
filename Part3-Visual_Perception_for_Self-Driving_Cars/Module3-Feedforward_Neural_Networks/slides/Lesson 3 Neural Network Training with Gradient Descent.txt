[MUSIC] So far in this module, we have reviewed what comprises
a feedforward neural network model, and how to evaluate the performance of a
neural network model using loss functions. This lesson will explain the final major
component of designing neural networks, the training process. Specifically, we will be
answering the following question. How can we get the best
parameter set theta for a feedforward neural network
given training data. The answer lies in using an iterative
optimization procedure with proper parameter initialization. Let us first revisit the feedforward
neural network training procedure we described previously. Given a training data input x and
the corresponding correct output, f star of x, we first pass the input
x through the hidden layers, then through the output layer
to get the final output y. We see here that the output y is
a function of the parameters theta. And remember,
that theta comprises the weights and the biases of our affine
transformations inside the network. Next, we compare our predicted output f
of x and theta with the correct output, f star of x through the loss function. Remember that the loss function measures
how large the error is between the network output and our true output. Our goal is to get a small value for the
loss function across the entire data set. We do so by using the loss function as
a guide to produce a new set of parameters theta that are expected to give
a lower value of the loss function. Specifically, we use the gradient
of the loss function to modify the parameters theta. This optimization procedure
is known as gradient descent. Before describing gradient
descent in detail, let's take another look at
the neural network loss function. Usually, we have thousands
of training example pairs, x and f star of x, available for
autonomous driving tasks. We can compute the loss
over all training examples, as the mean of the losses over
the individual training examples. We can then compute the gradient
of the training loss with respect to the parameters theta which
is equal to the mean of the gradient of the individual losses
over every training example. Here we use the fact that the gradient and
the sum are linear operators. So the gradient of a sum is equal to
the sum of the individual gradients. Using the formulated gradient equation, we can now describe the batch gradient
descent optimization algorithm. Batch gradient descent is a linear
first order optimization method. Iterative means that it starts from
an initial guess of parameters theta and improves on these parameters iteratively. First order means that the algorithm
only uses the first order derivative to improve the parameters theta. Batch gradient descent goes as follows. First, the parameters theta of
the neural network are initialized. Second, a stopping
condition is determined, which terminates the algorithm and
returns a final set of parameters. Once the iterative procedure begins, the first thing to be performed by the
algorithm is to compute the gradient of the loss function with respect to the
parameters theta, denoted del sub theta. The gradient can be computed using
the equation we derived earlier. Finally, the parameters theta are updated
according to the computed gradient. Here, epsilon is called the learning
rate and controls how much we adjust the parameters in the direction of
the negative gradient at every iteration. Let's take a look at a visual example of
batch gradient descent in the 2D case. Here, we are trying to find
the parameters theta one and theta two that minimize
our function J of theta. Theta is shaped like an oblong ball shown
here with contour lines of equal value. Gradient descent iteratively finds new
parameters theta that take us a step down the bowl at each iteration. The first step of the algorithm is
to initialize the parameters theta. Using our initial parameters,
we arrive at an initial value for our loss function denoted by the red dot. We start gradient descent by computing
the gradient of the loss function at the initial parameter
values theta 1 and theta 2. Using the update step, we then get the new parameters to arrive
at a lower point on our loss function. We repeat this process until we
achieve our stopping criteria. We then get the last set of
the parameters, theta 1 and theta 2 as our optimal set that
minimizes our loss function. Two pieces are still missing
from the presented algorithm. How do we initialize
the parameter's data and how do we decide when to
actually stop the algorithm? The answer to both of these questions is
still highly based on heuristics that work well in practice. For parameter initialization, we usually
initialized the weights using a standard normal distribution and
set the biases to 0. It is worth mentioning that there
are other heuristics specific to certain activation functions that
are widely used in a literature. We provide some of these heuristics
in a supplementary material. Defining the gradient descent's stopping
conditions is a bit more complex. There are three ways to determine
when to stop the training algorithm. Most simply,
we can decide to stop when a predefined maximum number of gradient
descent iterations is reached. Another heuristic is based on
how much the parameters theta changed between iterations. A small variation means the algorithm is
not updating the parameters effectively anymore, which might mean that
a minimum has been reached. The last widely used stopping criteria
is the change in the loss function value between iterations. Again, as the changes in the loss
function between iterations become small, the optimization is likely to
have converged to a minimum. Choosing one of these stopping
conditions is very much a matter of what works best for the problem at hand. We will revisit the stopping
conditions in the next lesson, as we study some of the pitfalls of the
training process, and how to avoid them. Unfortunately, the batch gradient descent
algorithm suffers from severe drawbacks. To be able to compute the gradient
we use backpropogation. Backpropogation involves computing
the output of the network for the example on which we would
like to evaluate the gradient. And batch gradient descent evaluates
the gradient over the whole training set. Making it very slow to
perform a single update step. Luckily, the laws function as well as
its gradient are means over the training dataset. For example, we know that the standard
error in a mean estimated from a set of N samples is sigma
over the square root of N. Where sigma is the standard
deviation of the distribution and N as the number of samples
used to estimate the mean. That means that the rate of decrease in
error in the gradient estimate is less than linear in the number of samples. This observation is very important,
as we now can use a small sub-sample of the training data or a mini batch
to compute our gradient estimate. So how does using mini batches modify
our batch gradient descent algorithm? The modification is actually quite simple. The only alteration to the base
algorithm is at the sampling step. Here we choose the sub sample n prime
of the training data as our mini batch. We can now evaluate the gradient and perform the update steps in an identical
manner to batch grading descent. This algorithm is called stochastic or
minibatch gradient descent, as we randomly select samples to include
in the minibatches at each iteration. However, this algorithm results in
an additional parameter to be determined, which is the size of the minibatch
that we want to use. To pick an appropriate minibatch, it has
to be noted that some kinds of hardware achieve better runtime with
specific sizes of data arrays. Specifically when using GPUs, it is common
to use power of two mini batch sizes which match GPU computing and
memory architecture as well. And therefore,
use the GPU resources efficiently. Let's look at some of the factors
that drive batch size selection. Multi-core architectures such as GPUs
are usually under-utilized by extremely small batch sizes, which motivates using
some absolute minimum batch size below which there's no reduction in
the time to process a minibatch. Furthermore, large batch sizes usually
provide a more accurate estimate of the gradient. Ensuring descent in a direction
that improves the network performance more reliably. However as noted previously, this improvement in the accuracy of
the estimate is less than linear. Small batch sizes on the other hand have
been seen to offer a regularlizing effect. With the best generalization often
seen at a batch size of one. If you're not sure what we mean
by generalization, don't worry. As we'll be exploring it more
closely in the next lesson. Furthermore, optimization algorithms
usually converge more quickly if they're allowed to rapidly compute approximate
estimates of the gradients and iterate more often rather than computing exact
gradients and performing fewer iterations. As a result of these trade-offs, typical
power of two mini batch sizes range from 32 to 256, with smaller sizes
sometimes being attempted for large models or to improve generalization. One final issue to keep in mind is the
requirement to shuffle the dataset before sampling the minibatch. Failing to shuffle the dataset at all can
reduce the effectiveness of your network. There exist many variants of stochastic
gradient descent in the literature, each having their own advantages and
disadvantages. It might be difficult to choose
which variant to use, and sometimes one of the variants works
better for certain problem than another. As a simple rule of thumb for autonomous driving applications, a safe
choice is the ADAM optimization method. It is quite robust to initial
parameters theta, and widely use. If you are interest in learning
more about this variance, have a look at the resources
listed in the supplemental notes. In this lesson,
you learned how to optimize the parameters of a neural network using
batch gradient descent. You also learned that there are a lot of
proposed variance of this optimization algorithm, with a safety
fault choice being ADAM. Congratulations, you've finished
the essential steps required to build and train an neural network. In the next lesson, we will discuss how
you can choose some of the optimization parameters to improve network training,
such as the learning rate. Also we'll discuss how to evaluate the
performance of our neural network using validation sets. See you next time. [MUSIC]