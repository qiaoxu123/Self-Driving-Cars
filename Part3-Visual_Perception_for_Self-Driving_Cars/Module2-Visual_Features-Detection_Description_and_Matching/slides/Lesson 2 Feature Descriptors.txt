[MUSIC] In the last video,
we described feature detection or the process of identifying feature
points in an image in a variety ways. However, remember that our end goal
is to use match features between two different images for
localization, object detection, and other perception tasks that require depth
estimation to points in the environment. To do so, we need to describe
features in a way that it allows for feature comparison to determine
the best match between frames. We therefore assign a descriptor to
every feature point in an image. In this video, you will learn what
makes a good feature descriptor for computer vision applications. You'll learn how to derive these
descriptors from images so we can use them in autonomous driving. Let's begin by defining what
a feature descriptor is. Mathematically, we define a feature
point by its coordinates u and v in the image frame. We describe a descriptor f as an n
dimensional vector associated with each feature. The descriptor has the task of providing
a summary of the image information in the vicinity of the feature itself,
and can take on many forms. Similar to the design of feature
detectors we also have some favorable characteristics required for
the design of descriptors to allow for robust feature matching. As with feature detectors descriptors
should be repeatable, that means that regardless of shifts in position,
scale, and illumination, the same point of interest in two images should
have approximately the same descriptor. This invariance in transformations
is one of the most researched topics when it
comes to descriptor design. And a large amount of work has been done
to provide descriptors that are invariant to scale, illumination, and
other variables in image formation. The second important characteristic of
a feature descriptor is distinctiveness. Two nearby features should
not have similar descriptors, as this will confuse our feature
matching process later on. Finally, descriptors should be compact and
efficient to compute. This is because we will usually require
matching to be performed in real time for autonomous driving applications. A wide variety of effective descriptors
have been developed for feature matching. So let's take a look at a specific case
study on the design of a single feature descriptors to give you sense for
descriptors work. We will describe how to compute the shift
features descriptors specifically designed by David Lowe in 1999. The procedure for computing shift
feature descriptors is as follows. Given a feature in the image,
the shift descriptor takes a 16 by 16 window of pixels around it, we call this
window the features local neighborhood. We then separate this window in to four, 4 by 4 cells such that each
cell contains 16 pixels. Next we compute the edges and
edge orientation of each pixel in each cell using the gradient
filters we discussed in module one. For stability of the descriptor, we suppress weak edges using a predefined
threshold as they are likely to vary significantly in orientation with
small amounts of noise between images. Finally, we compute a 32 dimensional
histogram of orientations for each cell. And concatenate the histograms for all
four cells to get a final 128 dimensional histogram for the feature at hand,
we call this histogram or descriptor. Some additional post processing is
done as well in that it helps the 128 dimensional vector retain stable
values under variable contrast, game, and other fundametric variations. SIFT is an example of a very well human
engineered feature descriptor, and is used in many state-of-the-art systems. It is usually computed over multiple
scales and orientations for better scale and rotation invariants. Finally, when combined with a scale
invariant feature detector, such as the difference
of Gaussian's detector, it results in a highly robust feature
detector and descriptor pair. It is worth mentioning that there
is huge literature out there for feature detectors and descriptors. The surf descriptive for
example uses similar concepts to SIFT while being significantly
faster to compute. Many other variants exist
in the literature including the Gradient Location-Orientation
Histogram or GLOH descriptor. The Binary Robust Independent
Elementary Features descriptor or BRIEF, and the Oriented Fast and
Rotated Brief descriptor or ORB. This is a lot of acronyms to remember, but don't worry, we don't expect
you to remember all of these. But you may see them in
the implementations available for use in
Open Source Computer Vision Libraries. We've now completed our discussion on
feature detectors and descriptors. Although most of the discussed algorithms
have open source implementations, some like SIFT and SURF are patented and should not be used commercially
without approval of the authors. Fortunately, the feature detector and
descriptor literature up there is vast and some really good algorithms such as ORB
match the performance of SIFT and SURF and are free to use even commercially. In this lesson, you learned what
comprises a feature descriptor, what characteristics are favorable
when designing these descriptors. And different algorithms that
are available in the open source libraries to extract feature
descriptors as you need them. In combination with the feature extractors
we talked about in the previous video, you're now ready to take
on the challenging tasks of matching features between images
using their computed descriptors. You'll learn more about
this in the next video. See you then. [MUSIC]