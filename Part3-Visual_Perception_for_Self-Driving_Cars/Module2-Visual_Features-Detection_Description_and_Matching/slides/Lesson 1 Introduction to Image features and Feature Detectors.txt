Hello again, and welcome to
the second week of the course. This week, we will be learning
about image features, which are distinctive points
of interest in the image. We use image features for several computer
vision applications in self-driving cars. For example, we can use these feature points to
localize our car across image frames or to get a global location in
a predefined map. All of these tasks share a general framework comprising
of feature detection, description, and
finally matching. In this video, we will
cover the first step of this framework namely
feature extraction. You will learn what comprises good image features and different algorithms that
perform feature extraction. Let's begin describing
this process by taking a real application,
image stitching. We're given two images from
two different cameras, and we would like to stitch them together to form a panorama. First, we need to identify distinctive points in our images. We call this point
image features. Second, we associate a descriptor for each feature from
its neighborhood. Finally, we use
these descriptors to match features across
two or more images. For our application here, we can use the matched features
in an image stitching algorithm to align the two images and create lovely panorama. Take a look at the details
of the stitched panorama. You can see the two images have been stitched
together from some of the artifacts at the edge of the image. So how do we do it? Don't worry. We'll explain each of the above three
steps in detail. But first, let's begin by defining what an image
feature really is. Features are points of
interest in an image. This definition is pretty vague, as it poses the
following question. What is considered an
interesting point? Points of interest
should be distinctive, and identifiable, and different from
its immediate neighborhood. Features should
also be repeatable. That means that we should
be able to extract the same features from two independent images
of the same scene. Third, features should be local. That means the features
should not change if an image region far away from the immediate
neighborhood changes. Forth, our features should
be abundant in an image. This is because many applications
such as calibration and localization require
a minimum number of distinctive points
to perform effectively. Finally, generating
features should not require a large amount
of computation, as it is usually used as a pre-processing step for the applications that
we've described. Take a look at
the following images. Can you think of pixels that abide by the above
characteristics? Repetitive texture less patches are very hard to localize. So these are definitely
not feature locations. You can see that
the two red rectangles located on the road
are almost identical. Patches with
large contrast change, where there's a strong gradient, are much easier to localize, but the patches along a certain image edge
might still be confusing. As an example,
the two red rectangles on the edge of the same lane
marking look very similar. So again, these are challenging locations to use as features. The easiest concept to localize in images
is that of a corner. A corner occurs when
the gradients in at least two significantly different directions are large. Examples of corners are
shown in the red rectangles. The most famous corner detector is the Harris Corner Detector, which uses image
gradient information to identify pixels that have a strong change in intensity
in both x and y directions. Many implementations
are available online in different
programming languages. However, the corners detected by Harris corner detectors
are not scale invariant, meaning that the corners can look different depending
on the distance the camera is away from the
object generating the corner. To remedy this problem, researchers proposed the
Harris-Laplace corner detector. Harris-Laplace detectors
detect corners at different scales and choose the best scale based on
the Laplacian of the image. Furthermore, researchers
have also been able to machine learn corners. One prominent algorithm,
the fast corner detector, is one of the most used
feature detectors due to its very high
computational efficiency and solid detection performance. Other scale invariant
feature detectors are based on the concept
of blobs such as the Laplacian of Gaussian
or the difference of Gaussian feature detectors of which there are many variance. We will not be discussing these detectors in
great depth here, as they represent a complex area
of ongoing research, but we can readily use a variety
of feature extractors. Thanks to robust open source
implementations like OpenCV. Now let's see some examples
of these detectors in action. Here you can see corners detected by the Harris
Corner Detector. The features primarily
capture corners as expected, where strong illumination
changes are visible. Here you can see Harris-Laplace features on the same image. By using the Laplacian
to determine scale, we can detect scale and
variant corners that can be more easily matched as a vehicle
moves relative to the scene. Scale is represented here by the size of the circle
around each feature. The larger the circle, the larger the principal
scale of that feature. In this lesson, you learn what characteristics are required for good image features. You also learn
the different methods that can be used to extract
image features. Most of these methods are
already implemented in many programming
languages including Python and C plus plus, and are ready for you
to use whenever needed. As a matter of fact, you will be using the OpenCV
Python implementation of the Harris-Laplace
corner detector for this week's
programming assignment. In the next video, we will learn about
the second step of our feature extraction framework, the feature descriptors.